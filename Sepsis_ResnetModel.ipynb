{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# System should have the following packages installed \n",
    "# python 3.7.6, keras 2.4.6, tensorflow 2.2, pydot, \n",
    "# graphviz, keras-resnet(not needed), imbalanced -learn\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from resnets_utils import *\n",
    "from keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 3\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value. You'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = (f,f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = layers.Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\", [3, 4, 4, 6])\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    A = identity_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = 'a')\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A], feed_dict={A_prev: X, K.learning_phase(): 0})\n",
    "    print(\"out = \" + str(out[0][1][1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(F2, (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    ##### SHORTCUT PATH #### (≈2 lines)\n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = layers.Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\", [3, 4, 4, 6])\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    A = convolutional_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = 'a')\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A], feed_dict={A_prev: X, K.learning_phase(): 0})\n",
    "    print(\"out = \" + str(out[0][1][1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape = (24, 24, 3), classes = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', \n",
    "               kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Stage 3 (≈4 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    # Stage 4 (≈6 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    # Stage 5 (≈3 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
    "    X = AveragePooling2D((2,2), padding=\"same\", name='avg_pool')(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50_model = ResNet50(input_shape = (24, 24, 3), classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from os import listdir\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pd.read_csv('C:/Users/nandh/OneDrive/Desktop/IndependantStudy/temp1/preprocessed_Train.csv', sep = ',')\n",
    "#preprocessed_data\n",
    "X = preprocessed_data.drop('Patient_id', axis=1)\n",
    "X = X.drop('SepsisLabel', axis=1)\n",
    "X = X.drop('Sepsis_Shift1', axis=1)\n",
    "X = X.drop('Sepsis_Shift6', axis=1)\n",
    "X = X.drop('Sepsis_Shift12', axis=1)\n",
    "#X\n",
    "y = preprocessed_data['SepsisLabel']\n",
    "#y\n",
    "y_Shift1 = preprocessed_data['Sepsis_Shift1']\n",
    "#y_Shift1\n",
    "y_Shift6 = preprocessed_data['Sepsis_Shift6']\n",
    "#y_Shift6\n",
    "y_Shift12 = preprocessed_data['Sepsis_Shift12']\n",
    "#y_Shift12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state= 109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.randn(X_train.shape[0], 24, 24, 3)\n",
    "X_test = np.random.randn(X_test.shape[0], 24, 24, 3)\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "#print (\"Y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "#print (\"Y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing one hot encoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# creating one hot encoder object \n",
    "onehotencoder = OneHotEncoder()\n",
    "#reshape the 1-D country array to 2-D as fit_transform expects 2-D and finally fit the object \n",
    "y_train = onehotencoder.fit_transform(y_train.values.reshape(-1,1)).toarray()\n",
    "#print (\"Y_train shape: \" + str(y_train.shape))\n",
    "y_test = onehotencoder.fit_transform(y_test.values.reshape(-1,1)).toarray()\n",
    "print (\"Y_train shape: \" + str(y_test.shape))\n",
    "print (\"Y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ResNet50_model.fit(X_train, y_train, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = ResNet50_model.evaluate(X_test, y_test)\n",
    "print (\"Loss = \" + str(pre[0]))\n",
    "print (\"Test Accuracy = \" + str(pre[1]))\n",
    "print(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(ResNet50_model.predict(X_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(np.argmax(y_test, axis=1), pred)\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = cmat[0,0]\n",
    "TP = cmat[0,1]\n",
    "FN = cmat[1,0]\n",
    "FP = cmat[1,1]\n",
    "print(TP,TN,FP,FN)\n",
    "sensitivity = TP/(TP+FN)\n",
    "print('Sensitivity : ', sensitivity )\n",
    "specificity = TN/(TN+FP)\n",
    "print('Specificity : ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE\n",
    "X_train_no, X_test_no, y_train_no, y_test_no = train_test_split(X, y, test_size=0.3, random_state= 109)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train_no==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train_no==0)))\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train_no, y_train_no.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"X_train shape: \" + str(X_train_res.shape))\n",
    "print (\"Y_train shape: \" + str(y_train_res.shape))\n",
    "print (\"X_test shape: \" + str(X_test_no.shape))\n",
    "print (\"Y_test shape: \" + str(y_test_no.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_res = np.random.randn(X_train_res.shape[0], 24, 24, 3)\n",
    "y_test_no = np.random.randn(X_test_no.shape[0], 24, 24, 3)\n",
    "print (\"X_train shape: \" + str(X_train_res.shape))\n",
    "print (\"X_test shape: \" + str(X_test_no.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing one hot encoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# creating one hot encoder object \n",
    "onehotencoder = OneHotEncoder()\n",
    "#reshape the 1-D country array to 2-D as fit_transform expects 2-D and finally fit the object \n",
    "y_train_smote = onehotencoder.fit_transform(y_train_res.reshape(-1,1)).toarray()\n",
    "#print (\"Y_train shape: \" + str(y_train.shape))\n",
    "y_test_smote = onehotencoder.fit_transform(y_test_no.reshape(-1,1)).toarray()\n",
    "print (\"Y_train shape: \" + str(y_train_smote.shape))\n",
    "print (\"Y_test shape: \" + str(y_test_smote.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_smote = ResNet50_model.fit(X_train_res, y_train_smote, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_smote = ResNet50_model.evaluate(X_test_no, y_test_smote)\n",
    "print (\"Loss = \" + str(pre_smote[0]))\n",
    "print (\"Test Accuracy = \" + str(pre_smote[1]))\n",
    "print(pre_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_smote = np.argmax(ResNet50_model.predict(X_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat_smote = confusion_matrix(np.argmax(y_test_smote, axis=1), pred_smote)\n",
    "cmat_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = cmat_smote[0,0]\n",
    "TP = cmat_smote[0,1]\n",
    "FN = cmat_smote[1,0]\n",
    "FP = cmat_smote[1,1]\n",
    "print(TP,TN,FP,FN)\n",
    "sensitivity = TP/(TP+FN)\n",
    "print('Sensitivity : ', sensitivity )\n",
    "specificity = TN/(TN+FP)\n",
    "print('Specificity : ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 hour Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y_Shift1, test_size=0.3, random_state= 109)\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train_1.shape))\n",
    "print (\"Y_train shape: \" + str(y_train_1.shape))\n",
    "print (\"X_test shape: \" + str(X_test_1.shape))\n",
    "print (\"Y_test shape: \" + str(y_test_1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = np.random.randn(X_train_1.shape[0], 24, 24, 3)\n",
    "X_test_1 = np.random.randn(X_test_1.shape[0], 24, 24, 3)\n",
    "print (\"X_train shape: \" + str(X_train_1.shape))\n",
    "#print (\"Y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test_1.shape))\n",
    "#print (\"Y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing one hot encoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# creating one hot encoder object \n",
    "onehotencoder = OneHotEncoder()\n",
    "#reshape the 1-D country array to 2-D as fit_transform expects 2-D and finally fit the object \n",
    "y_train_1hour = onehotencoder.fit_transform(y_train_1.values.reshape(-1,1)).toarray()\n",
    "#print (\"Y_train shape: \" + str(y_train.shape))\n",
    "y_test_1hour = onehotencoder.fit_transform(y_test_1.values.reshape(-1,1)).toarray()\n",
    "print (\"Y_train shape: \" + str(y_train_1hour.shape))\n",
    "print (\"Y_test shape: \" + str(y_test_1hour.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1hour = ResNet50_model.fit(X_train_1, y_train_1hour, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1h = ResNet50_model.evaluate(X_test_1, y_test_1hour)\n",
    "print (\"Loss = \" + str(pred_1h[0]))\n",
    "print (\"Test Accuracy = \" + str(pred_1h[1]))\n",
    "print(pred_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1hr = np.argmax(ResNet50_model.predict(X_test_1), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat_1hr = confusion_matrix(np.argmax(y_test_1hour, axis=1), pred_1hr)\n",
    "cmat_1hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = cmat_1hr[0,0]\n",
    "TP = cmat_1hr[0,1]\n",
    "FN = cmat_1hr[1,0]\n",
    "FP = cmat_1hr[1,1]\n",
    "print(TP,TN,FP,FN)\n",
    "sensitivity = TP/(TP+FN)\n",
    "print('Sensitivity : ', sensitivity )\n",
    "specificity = TN/(TN+FP)\n",
    "print('Specificity : ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE\n",
    "X_train_1hr, X_test_1hr, y_train_1hr, y_test_1hr = train_test_split(X, y_Shift1, test_size=0.3, random_state= 109)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train_1hr==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train_1hr==0)))\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train_res1, y_train_res1 = sm.fit_sample(X_train_1hr, y_train_1hr.ravel())\n",
    "\n",
    "print('After1OverSampling, the shape of train_X: {}'.format(X_train_res1.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res1.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res1==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res1==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"X_train shape: \" + str(X_train_res1.shape))\n",
    "print (\"Y_train shape: \" + str(y_train_res1.shape))\n",
    "print (\"X_test shape: \" + str(X_test_1hr.shape))\n",
    "print (\"Y_test shape: \" + str(y_test_1hr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res1h = np.random.randn(X_train_res1.shape[0], 24, 24, 3)\n",
    "X_test_1h = np.random.randn(X_test_1hr.shape[0], 24, 24, 3)\n",
    "print (\"X_train shape: \" + str(X_train_res1h.shape))\n",
    "print (\"X_test shape: \" + str(X_test_1h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "y_train_smote1hr = onehotencoder.fit_transform(y_train_res1.reshape(-1,1)).toarray()\n",
    "y_test_smote1hr = onehotencoder.fit_transform(y_test_1hr.values.reshape(-1,1)).toarray()\n",
    "print (\"Y_train shape: \" + str(y_train_smote1hr.shape))\n",
    "print (\"Y_test shape: \" + str(y_test_smote1hr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1hour = ResNet50_model.fit(X_train_res1h, y_train_smote1hr, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_smote_1h = ResNet50_model.evaluate(X_test_1h, y_test_smote1hr)\n",
    "print (\"Loss = \" + str(pred_smote_1h[0]))\n",
    "print (\"Test Accuracy = \" + str(pred_smote_1h[1]))\n",
    "print(pred_smote_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sm_1hr = np.argmax(ResNet50_model.predict(X_test_1h), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat_sm_1hr = confusion_matrix(np.argmax(y_test_smote1hr, axis=1), pred_sm_1hr)\n",
    "cmat_sm_1hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = cmat_sm_1hr[0,0]\n",
    "TP = cmat_sm_1hr[0,1]\n",
    "FN = cmat_sm_1hr[1,0]\n",
    "FP = cmat_sm_1hr[1,1]\n",
    "print(TP,TN,FP,FN)\n",
    "sensitivity = TP/(TP+FN)\n",
    "print('Sensitivity : ', sensitivity )\n",
    "specificity = TN/(TN+FP)\n",
    "print('Specificity : ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(np.argmax(y_test_smote1hr, axis=1), pred_sm_1hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE\n",
    "X_train_6hr, X_test_6hr, y_train_6hr, y_test_6hr = train_test_split(X, y_Shift6, test_size=0.3, random_state= 109)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train_6hr==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train_6hr==0)))\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train_res6, y_train_res6 = sm.fit_sample(X_train_6hr, y_train_6hr.ravel())\n",
    "\n",
    "print('After1OverSampling, the shape of train_X: {}'.format(X_train_res6.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res6.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res6==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res6==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"X_train shape: \" + str(X_train_res6.shape))\n",
    "print (\"Y_train shape: \" + str(y_train_res6.shape))\n",
    "print (\"X_test shape: \" + str(X_test_6hr.shape))\n",
    "print (\"Y_test shape: \" + str(y_test_6hr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res6h = np.random.randn(X_train_res6.shape[0], 24, 24, 3)\n",
    "X_test_6h = np.random.randn(X_test_6hr.shape[0], 24, 24, 3)\n",
    "print (\"X_train shape: \" + str(X_train_res6h.shape))\n",
    "print (\"X_test shape: \" + str(X_test_6h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "y_train_smote6hr = onehotencoder.fit_transform(y_train_res6.reshape(-1,1)).toarray()\n",
    "y_test_smote6hr = onehotencoder.fit_transform(y_test_6hr.values.reshape(-1,1)).toarray()\n",
    "print (\"Y_train shape: \" + str(y_train_smote6hr.shape))\n",
    "print (\"Y_test shape: \" + str(y_test_smote6hr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_6hour = ResNet50_model.fit(X_train_res6h, y_train_smote6hr, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_smote_6h = ResNet50_model.evaluate(X_test_6h, y_test_smote6hr)\n",
    "print (\"Loss = \" + str(pred_smote_6h[0]))\n",
    "print (\"Test Accuracy = \" + str(pred_smote_6h[1]))\n",
    "print(pred_smote_6h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sm_6hr = np.argmax(ResNet50_model.predict(X_test_6h), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat_sm_6hr = confusion_matrix(np.argmax(y_test_smote6hr, axis=1), pred_sm_6hr,labels=[0,1])\n",
    "cmat_sm_6hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = cmat_sm_6hr[0,0]\n",
    "TP = cmat_sm_6hr[0,1]\n",
    "FN = cmat_sm_6hr[1,0]\n",
    "FP = cmat_sm_6hr[1,1]\n",
    "print(TP,TN,FP,FN)\n",
    "sensitivity = TP/(TP+FN)\n",
    "print('Sensitivity : ', sensitivity )\n",
    "specificity = TN/(TN+FP)\n",
    "print('Specificity : ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(np.argmax(y_test_smote6hr, axis=1), pred_sm_6hr,labels = [1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE\n",
    "X_train_12hr, X_test_12hr, y_train_12hr, y_test_12hr = train_test_split(X, y_Shift12, test_size=0.3, random_state= 109)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train_12hr==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train_12hr==0)))\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train_res12, y_train_res12 = sm.fit_sample(X_train_12hr, y_train_12hr.ravel())\n",
    "\n",
    "print('After1OverSampling, the shape of train_X: {}'.format(X_train_res12.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res12.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res12==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res12==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"X_train shape: \" + str(X_train_res12.shape))\n",
    "print (\"Y_train shape: \" + str(y_train_res12.shape))\n",
    "print (\"X_test shape: \" + str(X_test_12hr.shape))\n",
    "print (\"Y_test shape: \" + str(y_test_12hr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res12h = np.random.randn(X_train_res12.shape[0], 24, 24, 3)\n",
    "X_test_12h = np.random.randn(X_test_12hr.shape[0], 24, 24, 3)\n",
    "print (\"X_train shape: \" + str(X_train_res12h.shape))\n",
    "print (\"X_test shape: \" + str(X_test_12h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "y_train_smote12hr = onehotencoder.fit_transform(y_train_res12.reshape(-1,1)).toarray()\n",
    "y_test_smote12hr = onehotencoder.fit_transform(y_test_12hr.values.reshape(-1,1)).toarray()\n",
    "print (\"Y_train shape: \" + str(y_train_smote12hr.shape))\n",
    "print (\"Y_test shape: \" + str(y_test_smote12hr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_12hour = ResNet50_model.fit(X_train_res12h, y_train_smote12hr, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_smote_12h = ResNet50_model.evaluate(X_test_12h, y_test_smote12hr)\n",
    "print (\"Loss = \" + str(pred_smote_12h[0]))\n",
    "print (\"Test Accuracy = \" + str(pred_smote_12h[1]))\n",
    "print(pred_smote_12h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sm_12hr = np.argmax(ResNet50_model.predict(X_test_12h), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat_sm_12hr = confusion_matrix(np.argmax(y_test_smote12hr, axis=1), pred_sm_12hr)\n",
    "cmat_sm_12hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = cmat_sm_12hr[0,0]\n",
    "TP = cmat_sm_12hr[0,1]\n",
    "FN = cmat_sm_12hr[1,0]\n",
    "FP = cmat_sm_12hr[1,1]\n",
    "print(TP,TN,FP,FN)\n",
    "sensitivity = TP/(TP+FN)\n",
    "print('Sensitivity : ', sensitivity )\n",
    "specificity = TN/(TN+FP)\n",
    "print('Specificity : ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(np.argmax(y_test_smote6hr, axis=1), pred_sm_6hr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
